{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn, loss\n",
    "\n",
    "from network.rtpose_model import get_model\n",
    "from training.datasets.coco import get_loader\n",
    "from mxboard import SummaryWriter    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'training/dataset/COCO/images'\n",
    "mask_dir = 'training/dataset/COCO/mask'\n",
    "logdir = 'logs'\n",
    "json_path = 'training/dataset/COCO/COCO.json'\n",
    "model_path = 'model_checkpoints/'\n",
    "lr = 1.                    \n",
    "momentum = 0.9\n",
    "epochs_ft = 5\n",
    "epochs_pre = 5\n",
    "wd = 0.0                    \n",
    "nesterov = False\n",
    "optim = 'sgd'\n",
    "gpuIDs = [0]\n",
    "batch_size = 8\n",
    "print_freq = 20\n",
    "load_model = 'nb_bn_dropout2_ft_mobilenet_pose_ft_7_float32.params'\n",
    "log_key = 'notebook_tests'\n",
    "model_trunk='mobilenet'\n",
    "dtype='float32'\n",
    "\n",
    "ctx = [mx.gpu(e) for e in gpuIDs] if gpuIDs[0] != -1 else [mx.cpu()]\n",
    "ctx = ctx[0] # single GPU for now\n",
    "\n",
    "params_transform = dict()\n",
    "params_transform['mode'] = 5\n",
    "# === aug_scale ===\n",
    "params_transform['scale_min'] = 0.8\n",
    "params_transform['scale_max'] = 1.2\n",
    "params_transform['scale_prob'] = 1\n",
    "params_transform['target_dist'] = 0.6\n",
    "# === aug_rotate ===\n",
    "params_transform['max_rotate_degree'] = 20\n",
    "\n",
    "# ===\n",
    "params_transform['center_perterb_max'] = 20\n",
    "\n",
    "# === aug_flip ===\n",
    "params_transform['flip_prob'] = 0.5\n",
    "\n",
    "params_transform['np'] = 56\n",
    "params_transform['sigma'] = 7.0\n",
    "params_transform['limb_width'] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def build_names():\n",
    "    names = []\n",
    "    for j in range(1, 7):\n",
    "        for k in range(1, 3):\n",
    "            names.append('loss_stage%d_L%d' % (j, k))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(saved_for_loss, heat_temp, heat_weight,\n",
    "               vec_temp, vec_weight):\n",
    "\n",
    "    names = build_names()\n",
    "    saved_for_log = OrderedDict()\n",
    "    total_loss = 0\n",
    "    loss_fn2 = gluon.loss.L2Loss()\n",
    "    loss_fn1 = gluon.loss.L1Loss()\n",
    "    for j in range(len(saved_for_loss)//2):\n",
    "        pred1 = saved_for_loss[2 * j] * vec_weight \n",
    "        gt1 = vec_temp * vec_weight\n",
    "        pred2 = saved_for_loss[2 * j + 1] * heat_weight \n",
    "        gt2 = heat_weight * heat_temp\n",
    "        # Compute losses\n",
    "        loss1 = loss_fn2(pred1, gt1)\n",
    "        loss2 = loss_fn2(pred2, gt2)\n",
    "        #loss1 = 0.5*loss_fn1(pred1, gt1)+0.5*loss_fn2(pred1, gt1)\n",
    "        #loss2 = 0.5*loss_fn1(pred2, gt2)+0.5*loss_fn2(pred2, gt2)\n",
    "\n",
    "        total_loss = total_loss + loss1\n",
    "        total_loss = total_loss + loss2\n",
    "        saved_for_log[names[2 * j]] = loss1.mean().asscalar()\n",
    "        saved_for_log[names[2 * j + 1]] = loss2.mean().asscalar()\n",
    "\n",
    "    saved_for_log['max_ht'] = saved_for_loss[-1][:, 0:-1, :, :].asnumpy().max()\n",
    "    saved_for_log['min_ht'] = saved_for_loss[-1][:, 0:-1, :, :].asnumpy().min()\n",
    "    saved_for_log['max_paf'] = saved_for_loss[-2].asnumpy().max()\n",
    "    saved_for_log['min_paf'] = saved_for_loss[-2].asnumpy().min()\n",
    "\n",
    "    return total_loss, saved_for_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(iterator, model, epoch, is_train=True, trainer_trunk=None, trainer_pose=None, dtype='float32'):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    model.cast(dtype)\n",
    "    meter_dict = {}\n",
    "    for name in build_names():\n",
    "        meter_dict[name] = AverageMeter()\n",
    "    meter_dict['max_ht'] = AverageMeter()\n",
    "    meter_dict['min_ht'] = AverageMeter()    \n",
    "    meter_dict['max_paf'] = AverageMeter()    \n",
    "    meter_dict['min_paf'] = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    for i, (img, heatmap_target, heat_mask, paf_target, paf_mask) in enumerate(iterator):\n",
    "        img = img.as_in_context(ctx).astype(dtype, copy=False)\n",
    "        heatmap_target = heatmap_target.as_in_context(ctx).astype(dtype, copy=False)\n",
    "        heat_mask = heat_mask.as_in_context(ctx).astype(dtype, copy=False)\n",
    "        paf_target = paf_target.as_in_context(ctx).astype(dtype, copy=False)\n",
    "        paf_mask = paf_mask.as_in_context(ctx).astype(dtype, copy=False)\n",
    "                \n",
    "        with autograd.record(is_train):\n",
    "            # compute output\n",
    "            _,saved_for_loss = model(img)\n",
    "            total_loss, saved_for_log = get_loss(saved_for_loss, heatmap_target, heat_mask,\n",
    "                   paf_target, paf_mask)\n",
    "        \n",
    "        for name,_ in meter_dict.items():\n",
    "            meter_dict[name].update(saved_for_log[name], img.shape[0])\n",
    "        losses.update(total_loss.astype('float32').mean().asscalar(), img.shape[0])\n",
    "\n",
    "        if is_train:\n",
    "            total_loss.backward()\n",
    "            if trainer_trunk is not None:\n",
    "                trainer_trunk.step(img.shape[0])\n",
    "            trainer_pose.step(img.shape[0])\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % print_freq == 0 and is_train:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'.format(epoch, i, len(iterator)))\n",
    "            print('Data time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'.format( batch_time=batch_time))\n",
    "            print('Loss {loss.val:.4f} ({loss.avg:.4f})'.format(loss=losses))\n",
    "            writer.add_scalar('data/max_ht', {log_key:meter_dict['max_ht'].avg}, i+epoch*len(iterator))\n",
    "            writer.add_scalar('data/max_paf', {log_key:meter_dict['max_paf'].avg}, i+epoch*len(iterator))\n",
    "            writer.add_scalar('data/loss', {log_key:losses.avg}, i+epoch*len(iterator)),\n",
    "            for name, value in meter_dict.items():\n",
    "                print('{name}: {loss.val:.4f} ({loss.avg:.4f})\\t'.format(name=name, loss=value))\n",
    "            writer.flush()\n",
    "    return losses.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "train dataset len: 121522\n",
      "val dataset len: 4873\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "# load data\n",
    "train_data = get_loader(json_path, data_dir,\n",
    "                        mask_dir, 368, 8,\n",
    "                        'vgg', batch_size, params_transform = params_transform, \n",
    "                        shuffle=True, training=True, num_workers=8)\n",
    "print('train dataset len: {}'.format(len(train_data._dataset)))\n",
    "\n",
    "# validation data\n",
    "valid_data = get_loader(json_path, data_dir, mask_dir, 368,\n",
    "                            8, preprocess='vgg', training=False,\n",
    "                            batch_size=batch_size, params_transform = params_transform, shuffle=False, num_workers=8)\n",
    "print('val dataset len: {}'.format(len(valid_data._dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(trunk=model_trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.collect_params().reset_ctx(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model != '':\n",
    "    model.load_parameters(os.path.join(model_path, load_model), ctx=ctx)\n",
    "model.hybridize(static_shape=True, static_alloc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training first with backbone fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_key='nb_bn_dropout3'\n",
    "writer = SummaryWriter(logdir=logdir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the  pre-trained weights for now\n",
    "trainer_trunk = gluon.Trainer(model.model0.collect_params('.*CPM.*'), 'sgd', {'learning_rate':lr, 'momentum': momentum, 'wd':wd}) if model_trunk == 'vgg19' else None \n",
    "trainer_pose = gluon.Trainer(model.collect_params('block.*'), 'sgd', {'learning_rate':lr, 'momentum': momentum, 'wd':wd}) \n",
    "                                                                                          \n",
    "    \n",
    "for epoch in range(epochs_pre):\n",
    "    # train for one epoch\n",
    "    train_loss = run_epoch(train_data, model, epoch, is_train=True, trainer_trunk=trainer_trunk, trainer_pose=trainer_pose)\n",
    "    model.save_parameters(os.path.join(model_path, log_key+'_'+model_trunk+'_pose_'+str(epoch)+'.params'))\n",
    "    # evaluate on validation set\n",
    "    val_loss = run_epoch(valid_data, model, epoch, is_train=False)  \n",
    "                  \n",
    "    writer.add_scalar('epoch/train_loss', {log_key: train_loss}, epoch)\n",
    "    writer.add_scalar('epoch/val_loss', {log_key: val_loss}, epoch)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim ='adam'\n",
    "lr = 0.001\n",
    "wd = 0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optim == 'sgd':\n",
    "    trainer_trunk = gluon.Trainer(model.model0.collect_params(), 'sgd', {'learning_rate':lr, 'momentum': momentum, 'wd':wd, 'multi_precision':(dtype=='float16')})\n",
    "    trainer_pose = gluon.Trainer(model.collect_params('block.*'), 'sgd', {'learning_rate':lr, 'momentum': momentum, 'wd':wd, 'multi_precision':(dtype=='float16')})\n",
    "elif optim == 'adam':\n",
    "    trainer_trunk = gluon.Trainer(model.model0.collect_params(), 'adam', {'learning_rate':lr, 'wd':wd, 'multi_precision':(dtype=='float16')})\n",
    "    trainer_pose = gluon.Trainer(model.collect_params('block.*'), 'adam', {'learning_rate':lr, 'wd':wd, 'multi_precision':(dtype=='float16')})\n",
    "else:\n",
    "    raise \"Unknown optim \" + optim\n",
    "log_key += '_ft'        \n",
    "\n",
    "for epoch in range(epochs_pre, epochs_pre+epochs_ft):\n",
    "    # train for one epoch\n",
    "    train_loss = run_epoch(train_data, model, epoch, is_train=True, trainer_trunk=trainer_trunk, trainer_pose=trainer_pose, dtype=dtype)\n",
    "    model.save_parameters(os.path.join(model_path, log_key+'_'+model_trunk+'_pose_ft_'+str(epoch)+'_'+dtype+'.params'))\n",
    "    # evaluate on validation set\n",
    "    val_loss = run_epoch(valid_data, model, epoch, is_train=False)  \n",
    "                                 \n",
    "    writer.add_scalar('epoch_ft/train_loss', {log_key: train_loss}, epoch)\n",
    "    writer.add_scalar('epoch_ft/val_loss', {log_key: val_loss}, epoch)                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the symbol for the heatmap branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(mx.sym.var('data'))[0][1].save('model_checkpoints/export_mobilenet-heatmap-symbol.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the parameters for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export('model_checkpoints/export_mobilenet', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the parameters for everything with `ignore_extra=True` and then exporting again to have a slimmer faster network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_heatmap = gluon.nn.SymbolBlock.imports(\n",
    "    symbol_file='model_checkpoints/export_mobilenet-heatmap-symbol.json',\n",
    "    input_names=['data'],\n",
    "    ctx=ctx)\n",
    "model_heatmap.load_parameters('model_checkpoints/export_mobilenet-0000.params', ctx=ctx, ignore_extra=True)\n",
    "model_heatmap.hybridize()\n",
    "model_heatmap.export('model_checkpoints/export_mobilenet_heatmap', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading again the small network from scratch and testing runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(2)\n",
    "model_heatmap = gluon.nn.SymbolBlock.imports(\n",
    "    symbol_file='model_checkpoints/export_mobilenet_heatmap-symbol.json',\n",
    "    param_file='model_checkpoints/export_mobilenet_heatmap-0000.params',\n",
    "    input_names=['data'],\n",
    "    ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_heatmap.hybridize(static_shape=True, static_alloc=True)\n",
    "out = model_heatmap(mx.nd.ones((1,3,368,368), ctx=ctx))\n",
    "out.wait_to_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 16 ms, total: 32 ms\n",
      "Wall time: 30.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_heatmap(mx.nd.ones((1,3,368,368), ctx=ctx)).wait_to_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Float16 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in model_heatmap.collect_params().items():\n",
    "    if not ('gamma' in key or 'beta' in key or 'running_mean' in key or 'running_var' in key):\n",
    "        item.cast('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 15.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_heatmap(mx.nd.ones((1,3,368,368), ctx=ctx, dtype='float16')).wait_to_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
