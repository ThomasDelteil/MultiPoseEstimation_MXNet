{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import importlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gluoncv as gcv\n",
    "from gluoncv.model_zoo import get_model\n",
    "from mxboard import SummaryWriter    \n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn, loss\n",
    "import numpy as np\n",
    "\n",
    "import multi_pose.models\n",
    "import multi_pose.datasets.coco_data\n",
    "importlib.reload(multi_pose.models)\n",
    "importlib.reload(multi_pose.datasets.coco_data)\n",
    "\n",
    "from multi_pose.models import build_model\n",
    "from multi_pose.datasets.coco_data import get_loader\n",
    "from multi_pose.utils import AverageMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/dataset/COCO/images'\n",
    "mask_dir = 'data/dataset/COCO/mask'\n",
    "logdir = 'logs'\n",
    "json_path = 'data/dataset/COCO/COCO.json'\n",
    "model_path = 'model_checkpoints/'\n",
    "lr = 0.01                    \n",
    "momentum = 0.9\n",
    "epochs_ft = 5\n",
    "epochs_pre = 5\n",
    "wd = 0.0                    \n",
    "nesterov = False\n",
    "optim = 'sgd'\n",
    "gpuIDs = [0]\n",
    "batch_size = 32\n",
    "print_freq = 20\n",
    "load_model = 'notebook_tests_resnet18_v1b_pose_0.params'\n",
    "log_key = 'notebook_tests'\n",
    "model_trunk='resnet18_v1b'\n",
    "dtype='float32'\n",
    "\n",
    "ctx = [mx.gpu(e) for e in gpuIDs] if gpuIDs[0] != -1 else [mx.cpu()]\n",
    "ctx = ctx[0] # single GPU for now\n",
    "\n",
    "params_transform = dict()\n",
    "params_transform['mode'] = 5\n",
    "# === aug_scale ===\n",
    "params_transform['scale_min'] = 0.8\n",
    "params_transform['scale_max'] = 1.2\n",
    "params_transform['scale_prob'] = 1\n",
    "params_transform['target_dist'] = 0.6\n",
    "# === aug_rotate ===\n",
    "params_transform['max_rotate_degree'] = 20\n",
    "\n",
    "# ===\n",
    "params_transform['center_perterb_max'] = 20\n",
    "\n",
    "# === aug_flip ===\n",
    "params_transform['flip_prob'] = 0.5\n",
    "\n",
    "params_transform['np'] = 56\n",
    "params_transform['sigma'] = 7.0\n",
    "params_transform['limb_width'] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_names():\n",
    "    names = []\n",
    "    for j in range(1, 7):\n",
    "        for k in range(1, 3):\n",
    "            names.append('loss_stage%d_L%d' % (j, k))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluation loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 8 if model_trunk == 'mobilenet' or model_trunk == 'vgg19' else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "# load data\n",
    "train_data = get_loader(json_path, data_dir, mask_dir, 384, downsample, batch_size, params_transform = params_transform, shuffle=True, training=True, num_workers=8)\n",
    "print('train dataset len: {}'.format(len(train_data._dataset)))\n",
    "\n",
    "# validation data\n",
    "valid_data = get_loader(json_path, data_dir, mask_dir, 384, downsample, training=False, batch_size=batch_size, params_transform = params_transform, shuffle=False, num_workers=8)\n",
    "print('val dataset len: {}'.format(len(valid_data._dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(trunk=model_trunk, pretrained_ctx=ctx, is_train=True, num_joints=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model != '':\n",
    "    model.load_parameters(os.path.join(model_path, load_model), ctx=ctx)\n",
    "model.hybridize(static_shape=True, static_alloc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training first with backbone fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(saved_for_loss, heat_temp, heat_weight,\n",
    "               vec_temp, vec_weight):\n",
    "\n",
    "    names = build_names()\n",
    "    saved_for_log = OrderedDict()\n",
    "    total_loss = 0\n",
    "    loss_fn = gluon.loss.L2Loss()\n",
    "    for j in range(len(saved_for_loss)//2):\n",
    "        pred1 = saved_for_loss[2 * j] * vec_weight \n",
    "        gt1 = vec_temp * vec_weight\n",
    "        pred2 = saved_for_loss[2 * j + 1] * heat_weight \n",
    "        gt2 = heat_weight * heat_temp\n",
    "\n",
    "        # Compute losses\n",
    "        loss1 = loss_fn(pred1, gt1)\n",
    "        loss2 = loss_fn(pred2, gt2)\n",
    "\n",
    "        total_loss = total_loss + loss1\n",
    "        total_loss = total_loss + loss2\n",
    "        saved_for_log[names[2 * j]] = loss1.mean().asscalar()\n",
    "        saved_for_log[names[2 * j + 1]] = loss2.mean().asscalar()\n",
    "\n",
    "    saved_for_log['max_ht'] = saved_for_loss[-1][:, 0:-1, :, :].asnumpy().max()\n",
    "    saved_for_log['min_ht'] = saved_for_loss[-1][:, 0:-1, :, :].asnumpy().min()\n",
    "    saved_for_log['max_paf'] = saved_for_loss[-2].asnumpy().max()\n",
    "    saved_for_log['min_paf'] = saved_for_loss[-2].asnumpy().min()\n",
    "\n",
    "    return total_loss, saved_for_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(iterator, model, epoch, is_train=True, trainer_trunk=None, trainer_pose=None, dtype='float32'):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    model.cast(dtype)\n",
    "    meter_dict = {}\n",
    "    for name in build_names():\n",
    "        meter_dict[name] = AverageMeter()\n",
    "    meter_dict['max_ht'] = AverageMeter()\n",
    "    meter_dict['min_ht'] = AverageMeter()    \n",
    "    meter_dict['max_paf'] = AverageMeter()    \n",
    "    meter_dict['min_paf'] = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    for i, (img, heatmap_target, heat_mask, paf_target, paf_mask) in enumerate(iterator):\n",
    "        img = img.as_in_context(ctx).astype(dtype, copy=False)\n",
    "        heatmap_target = heatmap_target.as_in_context(ctx).astype(dtype, copy=False)\n",
    "        heat_mask = heat_mask.as_in_context(ctx).astype(dtype, copy=False)\n",
    "        paf_target = paf_target.as_in_context(ctx).astype(dtype, copy=False)\n",
    "        paf_mask = paf_mask.as_in_context(ctx).astype(dtype, copy=False)\n",
    "                \n",
    "        with autograd.record(is_train):\n",
    "            # compute output\n",
    "            out = model(img)\n",
    "            if type(out[0]) == tuple: # vgg19 or mobilenet\n",
    "                total_loss, saved_for_log = get_loss(out[1], heatmap_target, heat_mask,\n",
    "                       paf_target, paf_mask)\n",
    "            else: # resnet\n",
    "                total_loss, saved_for_log = get_loss(list(out), heatmap_target, heat_mask,\n",
    "                       paf_target, paf_mask)\n",
    "        \n",
    "        for name,_ in saved_for_log.items():\n",
    "            meter_dict[name].update(saved_for_log[name], img.shape[0])\n",
    "        losses.update(total_loss.astype('float32').mean().asscalar(), img.shape[0])\n",
    "\n",
    "        if is_train:\n",
    "            total_loss.backward()\n",
    "            if trainer_trunk is not None:\n",
    "                trainer_trunk.step(img.shape[0])\n",
    "            trainer_pose.step(img.shape[0])\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % print_freq == 0 and is_train:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'.format(epoch, i, len(iterator)))\n",
    "            print('Data time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'.format( batch_time=batch_time))\n",
    "            print('Loss {loss.val:.4f} ({loss.avg:.4f})'.format(loss=losses))\n",
    "            writer.add_scalar('data/max_ht', {log_key:meter_dict['max_ht'].avg}, i+epoch*len(iterator))\n",
    "            writer.add_scalar('data/max_paf', {log_key:meter_dict['max_paf'].avg}, i+epoch*len(iterator))\n",
    "            writer.add_scalar('data/loss', {log_key:losses.avg}, i+epoch*len(iterator)),\n",
    "            for name in saved_for_log:\n",
    "                print('{name}: {loss.val:.4f} ({loss.avg:.4f})\\t'.format(name=name, loss=meter_dict[name]))\n",
    "            writer.flush()\n",
    "            print()\n",
    "    return losses.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_key=model_trunk+\"refactor\"\n",
    "writer = SummaryWriter(logdir=logdir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/3798]\t\n",
      "Data time 3.901 (3.901)\t\n",
      "Loss 0.0010 (0.0010)\n",
      "loss_stage1_L1: 0.0005 (0.0005)\t\n",
      "loss_stage1_L2: 0.0005 (0.0005)\t\n",
      "max_ht: 1.0735 (1.0735)\t\n",
      "min_ht: -0.0422 (-0.0422)\t\n",
      "max_paf: 1.1105 (1.1105)\t\n",
      "min_paf: -1.2107 (-1.2107)\t\n",
      "\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[02:08:39] src/storage/./pooled_storage_manager.h:145: cudaMalloc failed: out of memory\n\nStack trace returned 10 entries:\n[bt] (0) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x40e86a) [0x7f5e1f97886a]\n[bt] (1) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x40eea1) [0x7f5e1f978ea1]\n[bt] (2) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x361c5e3) [0x7f5e22b865e3]\n[bt] (3) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x3620ccf) [0x7f5e22b8accf]\n[bt] (4) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e21976) [0x7f5e2238b976]\n[bt] (5) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e21c27) [0x7f5e2238bc27]\n[bt] (6) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#3}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x315) [0x7f5e2238ca85]\n[bt] (7) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#4}>::_M_invoke(std::_Any_data const&, mxnet::RunContext)+0x4d) [0x7f5e2238d18d]\n[bt] (8) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d78cfc) [0x7f5e222e2cfc]\n[bt] (9) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d79a67) [0x7f5e222e3a67]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-026e6c908f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_trunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer_trunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_pose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer_pose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_key\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_trunk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pose2_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-5967b0a0b3f0>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(iterator, model, epoch, is_train, trainer_trunk, trainer_pose, dtype)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# resnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 total_loss, saved_for_log = get_loss(list(out), heatmap_target, heat_mask,\n\u001b[0;32m---> 30\u001b[0;31m                        paf_target, paf_mask)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msaved_for_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-779abb3c5116>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(saved_for_loss, heat_temp, heat_weight, vec_temp, vec_weight)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msaved_for_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0msaved_for_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1993\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \"\"\"\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [02:08:39] src/storage/./pooled_storage_manager.h:145: cudaMalloc failed: out of memory\n\nStack trace returned 10 entries:\n[bt] (0) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x40e86a) [0x7f5e1f97886a]\n[bt] (1) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x40eea1) [0x7f5e1f978ea1]\n[bt] (2) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x361c5e3) [0x7f5e22b865e3]\n[bt] (3) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x3620ccf) [0x7f5e22b8accf]\n[bt] (4) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e21976) [0x7f5e2238b976]\n[bt] (5) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e21c27) [0x7f5e2238bc27]\n[bt] (6) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#3}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x315) [0x7f5e2238ca85]\n[bt] (7) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#4}>::_M_invoke(std::_Any_data const&, mxnet::RunContext)+0x4d) [0x7f5e2238d18d]\n[bt] (8) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d78cfc) [0x7f5e222e2cfc]\n[bt] (9) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d79a67) [0x7f5e222e3a67]\n\n"
     ]
    }
   ],
   "source": [
    "# Fix the  pre-trained weights for now\n",
    "if model_trunk == 'vgg19':\n",
    "    trainer_trunk = gluon.Trainer(model.model0.collect_params('.*CPM.*'), 'sgd', {'learning_rate':lr, 'momentum': momentum, 'wd':wd})\n",
    "    trainer_pose = gluon.Trainer(model.collect_params('block.*'), 'sgd', {'learning_rate':lr, 'momentum': momentum, 'wd':wd}) \n",
    "elif model_trunk =='mobilenet':\n",
    "    trainer_pose = gluon.Trainer(model.collect_params('block.*'), 'sgd', {'learning_rate':lr, 'momentum': momentum, 'wd':wd}) \n",
    "    trainer_trunk = None\n",
    "elif 'resnet' in model_trunk:\n",
    "    trainer_trunk = gluon.Trainer(model.collect_params('.*resnet.*'), 'adam', {'learning_rate':0.00005})\n",
    "    trainer_pose = gluon.Trainer(model.collect_params('.*final.*'), 'adam', {'learning_rate':0.00005}) \n",
    "\n",
    "                                                                                          \n",
    "    \n",
    "for epoch in range(epochs_pre):\n",
    "    # train for one epoch\n",
    "    train_loss = run_epoch(train_data, model, epoch, is_train=True, trainer_trunk=trainer_trunk, trainer_pose=trainer_pose)\n",
    "    model.save_parameters(os.path.join(model_path, log_key+'_'+model_trunk+'_pose2_'+str(epoch)+'.params'))\n",
    "    # evaluate on validation set\n",
    "    val_loss = run_epoch(valid_data, model, epoch, is_train=False)  \n",
    "                  \n",
    "    writer.add_scalar('epoch/train_loss', {log_key: train_loss}, epoch)\n",
    "    writer.add_scalar('epoch/val_loss', {log_key: val_loss}, epoch)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim ='adam'\n",
    "lr = 0.001\n",
    "wd = 0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_trunk == 'vgg19':\n",
    "    trainer_trunk = gluon.Trainer(model.model0.collect_params('.*vgg19_.*'), 'adam', {'learning_rate':lr*0.1, 'wd':wd})\n",
    "    trainer_pose = gluon.Trainer(model.collect_params('block.*'), 'adam', {'learning_rate':lr*0.1, 'wd':wd}) \n",
    "elif model_trunk =='mobilenet':\n",
    "    trainer_trunk =  gluon.Trainer(model.model0.collect_params('.*mobilenet.*'), 'adam', {'learning_rate':lr*0.1, 'wd':wd})\n",
    "    trainer_pose = gluon.Trainer(model.collect_params('block.*'), 'adam', {'learning_rate':lr*0.1,  'wd':wd}) \n",
    "elif 'resnet' in model_trunk:\n",
    "    trainer_trunk = gluon.Trainer(model.collect_params('.*resnet.*'), 'adam', {'learning_rate':lr*0.1})\n",
    "    trainer_pose = gluon.Trainer(model.collect_params('.*final.*'), 'adam', {'learning_rate':lr*0.1})   \n",
    "\n",
    "log_key += '_ft'        \n",
    "\n",
    "for epoch in range(epochs_pre, epochs_pre+epochs_ft):\n",
    "    # train for one epoch\n",
    "    train_loss = run_epoch(train_data, model, epoch, is_train=True, trainer_trunk=trainer_trunk, trainer_pose=trainer_pose, dtype=dtype)\n",
    "    model.save_parameters(os.path.join(model_path, log_key+'_'+model_trunk+'_pose_ft_'+str(epoch)+'_'+dtype+'.params'))\n",
    "    # evaluate on validation set\n",
    "    val_loss = run_epoch(valid_data, model, epoch, is_train=False)  \n",
    "                                 \n",
    "    writer.add_scalar('epoch_ft/train_loss', {log_key: train_loss}, epoch)\n",
    "    writer.add_scalar('epoch_ft/val_loss', {log_key: val_loss}, epoch)                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the symbol for the heatmap branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(mx.sym.var('data'))[0][1].save('model_checkpoints/export_mobilenet-heatmap-symbol.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the parameters for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export('model_checkpoints/export_mobilenet', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the parameters for everything with `ignore_extra=True` and then exporting again to have a slimmer faster network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_heatmap = gluon.nn.SymbolBlock.imports(\n",
    "    symbol_file='model_checkpoints/export_mobilenet-heatmap-symbol.json',\n",
    "    input_names=['data'],\n",
    "    ctx=ctx)\n",
    "model_heatmap.load_parameters('model_checkpoints/export_mobilenet-0000.params', ctx=ctx, ignore_extra=True)\n",
    "model_heatmap.hybridize()\n",
    "model_heatmap.export('model_checkpoints/export_mobilenet_heatmap', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading again the small network from scratch and testing runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(2)\n",
    "model_heatmap = gluon.nn.SymbolBlock.imports(\n",
    "    symbol_file='model_checkpoints/export_mobilenet_heatmap-symbol.json',\n",
    "    param_file='model_checkpoints/export_mobilenet_heatmap-0000.params',\n",
    "    input_names=['data'],\n",
    "    ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_heatmap.hybridize(static_shape=True, static_alloc=True)\n",
    "out = model_heatmap(mx.nd.ones((1,3,368,368), ctx=ctx))\n",
    "out.wait_to_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 16 ms, total: 32 ms\n",
      "Wall time: 30.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_heatmap(mx.nd.ones((1,3,368,368), ctx=ctx)).wait_to_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Float16 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in model_heatmap.collect_params().items():\n",
    "    if not ('gamma' in key or 'beta' in key or 'running_mean' in key or 'running_var' in key):\n",
    "        item.cast('float16')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
